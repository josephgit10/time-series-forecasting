# Timeâ€‘Series ForecastingÂ forÂ SupplyÂ ChainÂ Logistics

> **Multiâ€‘store weekly demand forecasting with Prophet, LSTM, XGBoost, Airflow orchestration, and a FastAPI microâ€‘service â€“ fully containerised with Docker.**

---

## âœ¨ Key Features

| Area                | Tech                                          | Highlights                                                                     |
| ------------------- | --------------------------------------------- | ------------------------------------------------------------------------------ |
| **DataÂ prep**       | `pandas`, `numpy`                             | ETL script normalises three raw Walmartâ€‘style CSVs into a single weekly panel. |
| **Models**          | **Prophet**, **LSTM/Keras**, **XGBoost**      | Switchable backâ€‘ends, hyperâ€‘param search viaÂ KerasÂ Tuner /Â Optuna.             |
| **Orchestration**   | **ApacheÂ AirflowÂ 2.9**                        | DAG schedules daily refresh â†’ preprocessing Â· training Â· inference Â· upload.   |
| **Serving**         | **FastAPI + Uvicorn**                         | `/predict` endpoint returns forecast for any (Store,Â Date).                    |
| **Reproducibility** | Docker, `requirements.txt`,Â `environment.yml` | Oneâ€‘command spinâ€‘up viaÂ `dockerâ€‘compose`.                                      |
| **Dashboard**       | PlotlyÂ Dash (optional)                        | ActualÂ vsÂ Forecast lines, MAE rank bar, error heatâ€‘map.                        |

---

## ðŸš€ QuickÂ start

```bash
# 1Â Clone
$ git clone https://github.com/<you>/time-series-forecasting.git
$ cd time-series-forecasting

# 2Â CreateÂ env
$ python -m venv venv && source venv/bin/activate  # or use conda
$ pip install -r requirements.txt

# 3Â Add raw data (NOT tracked by Git)
$ cp ~/Downloads/*.csv data/raw/

# 4Â Preprocess â†’ merged_data.csv
$ python src/preprocessing.py

# 5Â Generate Prophet forecasts â†’ forecast_results.csv
$ python scripts/regenerate_forecasts.py

# 6Â Spinâ€‘up full stack (AirflowÂ +Â API)
$ docker-compose up --build
```

### Airflow

* WebÂ UI: [http://localhost:8080](http://localhost:8080) Â (default creds inÂ `docker-compose.yml`)
* DAG: **daily\_forecast\_pipeline**Â â†’ preprocess â†’ forecast â†’ â€¦

### FastAPI service

```
GET /health # â†’ {"status":"ok"}
GET /forecast/{store_id} # e.g., /forecast/3
# â†’ Returns a JSON array of all weekly forecasts for the given store.
```

Running standalone (without Docker):

```bash
uvicorn src.api:app --reload --port 8000
```

### PlotlyÂ Dash (optional)

```bash
python app.py   # localhost:8050
```

---

## ðŸ§°Â Development & Tests

```bash
# lint & tests
black src scripts airflow/dags
pytest -q
```

Create a feature branch âžœ PR to `main`; CI (GitHubÂ Actions) runs styleâ€‘check & unit tests.

---

## ðŸ“œÂ Data Folder Policy

`data/raw/` &Â `data/processed/` stay **out of Git**. Add your CSVs locally or mount via DockerÂ volume. See `data/README.md` for column documentation.

---

## ðŸ”§ Configuration

All tweakables (paths, model params) live in **`src/config.py`**. Environment variables override sensitive fields (DBÂ URLs, secrets). Example:

```bash
export FASTAPI_SECRET="superâ€‘secretâ€‘token"
export AIRFLOW__CORE__FERNET_KEY="..."
```

---

## ðŸª„Â Makefiles / Convenience scripts

```bash
make preprocess   # runs src/preprocessing.py
make forecast     # regenerates Prophet forecasts
make airflow-up   # docker-compose up airflow stack
make api          # runs FastAPI locally for dev
```

*(Requires GNUÂ Make on macOS/Linux; optional on Windows.)*

---
